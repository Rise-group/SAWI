{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f2097e-7481-4e90-9671-966368ea2cbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Graph Data\n",
    "\n",
    "This notebook organizes some input information and generate the necessary data to create a graph for a territory.\n",
    "\n",
    "#### The input information contains:\n",
    "1. The first part is 2 shapefiles that contains the links and nodes of the street network for 79 territories of Colombia. This information was downloaded from the OpenStreetMap database created by Geoff Boeing\n",
    "2. The second part is a shapefile that contains all the blocks of Colombia, which are going to be the origins of the graph.\n",
    "3. The last part is a shapefile that contains all the equipments in Colombia, which are going to be the destinations of the graph.\n",
    "\n",
    "<center><img src=\"img/Input_information_sample.png\" alt=\"files\" style=\"width:400px\"></center>\n",
    "\n",
    "**Note:** The script change the coordinate system of all the files to epsg:32618 for internal processing, but at the end all the files are exported in epsg:4326."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195eaa7f-7f1c-40e5-98b2-85d2c50bbfb5",
   "metadata": {},
   "source": [
    "# Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092ec1b-4bb9-457f-83c8-8b088f1763ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely as sh\n",
    "import os\n",
    "\n",
    "import utils.analysis as an"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44813f8-20bb-49ea-9df8-bfe5ded02c53",
   "metadata": {},
   "source": [
    "# Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fffd13d-e6ea-42e9-9b1c-9eec5b304550",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518e6cd-a6aa-4040-ac70-ee61b0388a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the names of the territories\n",
    "l_names = pd.read_csv('../data/input/table/mpios_names.txt',header=None)\n",
    "\n",
    "# Import equipments\n",
    "equipments = gpd.read_file('../data/output/shape/colombia_equipments_ID/colombia_equipments_ID.shp')\n",
    "equipments = equipments.to_crs('epsg:32618')\n",
    "\n",
    "# Define the desired codes for each equipment\n",
    "health_list = ['021001','021002','021003','021004','021011']           # 0210\n",
    "sport_list = ['020601','020602','020603','020604','020605','020606']   # 0206\n",
    "education_list = ['020903','020905']                                   # 0209\n",
    "financial_list = ['020501','020502']                                   # 0205\n",
    "cultural_list = ['020701','020702','020706','020709','020710']         # 0207\n",
    "parks_list = ['021101','021102','021104','021105']                     # 0211\n",
    "codes_list = health_list+sport_list+education_list+financial_list+cultural_list+parks_list\n",
    "# Filter equipmets based on the code_list\n",
    "equipments = equipments[equipments['CSIMBOL'].isin(codes_list)].reset_index(drop=True)\n",
    "\n",
    "# Import perimeters and all blocks\n",
    "perimeters = gpd.read_file('../data/input/shape/Colombia_urban_perimeters_name_pop/Colombia_urban_perimeters_name_pop.shp')\n",
    "perimeters = perimeters.to_crs('epsg:32618')\n",
    "all_blocks = gpd.read_file('../data/input/shape/MANZANA/MGN_URB_MANZANA.shp')\n",
    "all_blocks = all_blocks.to_crs('epsg:32618')\n",
    "\n",
    "# Import population data\n",
    "DANE_data = pd.read_csv('../data/input/table/DANE_2018_personas_manz.txt',low_memory=False)\n",
    "pop = DANE_data[['MANZ_CCNCT','poblacion']]\n",
    "\n",
    "# Import speed data\n",
    "speed_data = pd.read_csv('../data/input/table/slope_velocity_data.csv',sep=';')\n",
    "speed_data = speed_data.sort_index(ascending=False)\n",
    "speed_data = speed_data.reset_index(drop=True)\n",
    "\n",
    "for m in l_names:\n",
    "    \n",
    "    edges = gpd.read_file(f'../data/input/shape/edges/{m}_edges.shp')\n",
    "    edges = edges.to_crs('epsg:32618')\n",
    "    nodes = gpd.read_file(f'../data/input/shape/nodes/{m}_nodes.shp')\n",
    "    nodes['osmid'] = nodes['osmid'].astype(int).astype(str)\n",
    "    nodes = nodes.rename(columns={'osmid':'ID'})\n",
    "    nodes = nodes.to_crs('epsg:32618')\n",
    "    \n",
    "    ## Get perimeter from the network\n",
    "    # We make a join where we add to the edges gdf the row of the perimeters gdf that intersects each line of the edges gdf\n",
    "    join = edges.sjoin(perimeters, how='inner', predicate='intersects')\n",
    "    # Unite the perimeters in only one\n",
    "    filtered_perimeters = perimeters[perimeters['ID_mpio'].isin(join['ID_mpio'].unique())]\n",
    "    perimeter = gpd.GeoDataFrame(geometry=[filtered_perimeters.union_all()], crs=filtered_perimeters.crs)\n",
    "    # Give a name and an ID to the perimeter\n",
    "    perimeter['ID_p'] = m.split('-')[-1]\n",
    "    perimeter['Name'] = m.split('-')[0]\n",
    "    perimeter[['ID_p', 'Name', 'geometry']]\n",
    "\n",
    "    ## Get blocks by perimeter\n",
    "    join = all_blocks.sjoin(perimeter,how=\"inner\",predicate='intersects')\n",
    "    blocks = join.drop(['index_right','ID_p','Name'],axis=1)\n",
    "\n",
    "    ## Get destinations by perimeter\n",
    "    join = equipments.sjoin(perimeter,how=\"inner\",predicate='intersects')\n",
    "    destinations = join.drop(['index_right','ID_p','Name'],axis=1)\n",
    "    \n",
    "    ## Create blocks' centroid\n",
    "    # Create de controids of each polygon\n",
    "    blocksDissolved = blocks.dissolve('MANZ_CCNCT')\n",
    "    centroids = blocksDissolved.centroid\n",
    "    # Convert the GeoSeries file into a GeoDataFrame\n",
    "    origins = gpd.GeoDataFrame(geometry=gpd.GeoSeries(centroids))\n",
    "    origins.index.name = 'ID'\n",
    "    origins = origins.reset_index()\n",
    "\n",
    "    ## Found the nearest node from the network to each origin\n",
    "    nearest_origin = an.ckdnearest(origins,nodes)\n",
    "\n",
    "    ## Create a link that connect the origins with the nearest node\n",
    "    nearest_origin = nearest_origin.rename(columns={'elevation_to':'elevation'})\n",
    "    origins_links = an.create_links(nearest_origin, False, origins.crs)\n",
    "    \n",
    "    ## Get origins elevation by buffer\n",
    "    # (gdf_centroids, gdf_nodes, buffer_range, nearest, elevation_name='elevation', kind='centroid')\n",
    "    origins['elevation'] = origins.geometry.apply(lambda geom: \n",
    "        nodes.loc[nodes.within(geom.buffer(100)), 'elevation'].mean()\n",
    "        if len(nodes.loc[nodes.within(geom.buffer(100))]) > 0\n",
    "        else nearest_origin.loc[nearest_origin[f'geometry_from'].centroid == geom, 'elevation'].values[0])\n",
    "    \n",
    "    ## Get origins links grade\n",
    "    origins_links = an.get_links_grade(origins_links, origins, nodes)\n",
    "    \n",
    "    ## Found the nearest node from the network to each destination\n",
    "    nearest_destination = an.ckdnearest(destinations,nodes)\n",
    "\n",
    "    ## Create a link that connect the destinations with the nearest node\n",
    "    nearest_destination = nearest_destination.rename(columns={'elevation_to':'elevation'})\n",
    "    destinations_links = an.create_links(nearest_destination, False, destinations.crs)\n",
    "\n",
    "    ## Get destinations elevation by buffer\n",
    "    destinations['elevation'] = destinations.geometry.apply(lambda geom: \n",
    "        nodes.loc[nodes.within(geom.buffer(100)), 'elevation'].mean()\n",
    "        if len(nodes.loc[nodes.within(geom.buffer(100))]) > 0\n",
    "        else nearest_destination.loc[nearest_destination[f'geometry_from'].centroid == geom, 'elevation'].values[0])\n",
    "\n",
    "    ## Get destinations links grade\n",
    "    destinations_links = an.get_links_grade(destinations_links, destinations, nodes)\n",
    "\n",
    "    ## Assing walking speed to every link\n",
    "    edges = an.assign_walk_speed_Naismith_Langmuir(edges, 'km/h')\n",
    "    origins_links = an.assign_walk_speed_Naismith_Langmuir(origins_links, 'km/h')\n",
    "    destinations_links = an.assign_walk_speed_Naismith_Langmuir(destinations_links, 'km/h')\n",
    "\n",
    "    # Calculate the time [min] that it's necesary to walk througt\n",
    "    edges['weight'] = edges['length']/(edges['speed']*1000/60)\n",
    "    origins_links['weight'] = origins_links['length']/(origins_links['speed']*1000/60)\n",
    "    destinations_links['weight'] = destinations_links['length']/(destinations_links['speed']*1000/60)\n",
    "\n",
    "    edges = edges.to_crs('epsg:4326')\n",
    "    nodes = nodes.to_crs('epsg:4326')\n",
    "    origins = origins.to_crs('epsg:4326')\n",
    "    origins_links = origins_links.to_crs('epsg:4326')\n",
    "    destinations = destinations.to_crs('epsg:4326')\n",
    "    destinations_links = destinations_links.to_crs('epsg:4326')\n",
    "    perimeter = perimeter.to_crs('epsg:4326')\n",
    "    blocks = blocks.to_crs('epsg:4326')\n",
    "    \n",
    "    nodes = an.organize_nodes(nodes)\n",
    "    edges = an.organize_edges(edges)\n",
    "    origins = an.organize_origins(origins)\n",
    "    origins_links = an.organize_origins_links(origins_links)\n",
    "    destinations = an.organize_destinations(destinations,blocks,origins)\n",
    "    destinations_links = an.organize_destinations_links(destinations_links,destinations)\n",
    "    \n",
    "    # Add the population\n",
    "    origins = origins.set_index('ID').join(pop.set_index('MANZ_CCNCT'))\n",
    "    origins = origins.reset_index()\n",
    "    origins = origins.rename(columns={'poblacion':'pop'})\n",
    "    origins = origins[['ID','coord_X','coord_Y','coord_Z','Type','pop','geometry']]\n",
    "    # Change the NaN values for 0.0\n",
    "    origins = origins.fillna(0.0)\n",
    "    \n",
    "    graph_nodes = pd.concat([nodes,origins,destinations]).reset_index(drop=True)\n",
    "    graph_links = pd.concat([edges,origins_links,destinations_links]).reset_index(drop=True)\n",
    "\n",
    "    edges.to_file(f'../data/output/shape/edges/{m}_edges.shp')\n",
    "    nodes.to_file(f'../data/output/shape/nodes/{m}_nodes.shp')\n",
    "    origins.to_file(f'../data/output/shape/origins/{m}_origins.shp')\n",
    "    origins_links.to_file(f'../data/output/shape/origins_links/{m}_origins_links.shp')\n",
    "    destinations.to_file(f'../data/output/shape/destinations_/{m}_destinations.shp')\n",
    "    destinations_links.to_file(f'../data/output/shape/destinations_links_/{m}_destinations_links.shp')\n",
    "    perimeter.to_file(f'../data/output/shape/perimeters/{m}_perimeter.shp')\n",
    "    blocks.to_file(f'../data/output/shape/blocks/{m}_blocks.shp')\n",
    "    \n",
    "    graph_nodes.to_file(f'../data/output/shape/graphs_/nodes/{m}_graph_nodes.shp')\n",
    "    graph_links.to_file(f'../data/output/shape/graphs_/links/{m}_graph_links.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox_GIZ_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
